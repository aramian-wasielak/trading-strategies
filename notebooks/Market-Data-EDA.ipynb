{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Data EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this notebook we are going to take a look at the following Kaggle dataset:\n",
    "\n",
    "https://www.kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs\n",
    "\n",
    "The plan is to analyze this data and perform some quality checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/.conda/envs/trading-strategies/lib/python3.7/site-packages/pandas_datareader/compat/__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "pd.set_option('display.min_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data location\n",
    "DATA_DIR = '../data/price-volume-data-for-all-us-stocks-etfs/Data'\n",
    "STOCK_DIR = 'Stocks'\n",
    "ETF_DIR = 'ETFs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic functions to get stock & etf tickers and extract data from csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquityData:\n",
    "    \"\"\"\n",
    "    Provides an access layer to market data files made available\n",
    "    as part of the following Kaggle dataset:\n",
    "\n",
    "        https://www.kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs\n",
    "\n",
    "    Each file contains market data for one ticker (either stock or ETF). Also, each filename\n",
    "    has a specific naming convention: it consists of a ticker and a suffix (common to all files).\n",
    "\n",
    "    Attributes:\n",
    "        stock_dir (str): Directory location for stock files.\n",
    "        etf_dir (str): Directory location for ETF files.\n",
    "        file_suffix (str): A file suffix used by stock & ETF files.\n",
    "    \"\"\"\n",
    "\n",
    "    FILE_SUFFIX = '.us.txt'\n",
    "\n",
    "    def __init__(self, data_dir, stock_dir, etf_dir, file_suffix=FILE_SUFFIX):\n",
    "\n",
    "        self.stock_dir = os.path.join(data_dir, stock_dir)\n",
    "        self.etf_dir = os.path.join(data_dir, etf_dir)\n",
    "        self.file_suffix = file_suffix\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_tickers(dirname, file_suffix):\n",
    "        \"\"\"\n",
    "        Creates a list of available tickers in a given directory.\n",
    "\n",
    "        Only non-empty files are considered.\n",
    "\n",
    "        Args:\n",
    "            file_suffix (str): A file suffix used by each data file.\n",
    "\n",
    "        Returns:\n",
    "            (List[str]): A list of available tickers.\n",
    "        \"\"\"\n",
    "\n",
    "        tickers = []\n",
    "        for file in os.listdir(dirname):\n",
    "            # This dataset includes empty files which we exclude here.\n",
    "            if os.stat(os.path.join(dirname, file)).st_size > 0:\n",
    "                tickers.append(file.replace(file_suffix, ''))\n",
    "\n",
    "        return tickers\n",
    "\n",
    "    def get_etf_tickers(self):\n",
    "        \"\"\"\n",
    "        Returns a list of available ETF tickers.\n",
    "\n",
    "        Returns:\n",
    "            (List[str]): A sorted list of available ETF tickers.\n",
    "        \"\"\"\n",
    "\n",
    "        return sorted(self._get_tickers(self.etf_dir, self.file_suffix))\n",
    "\n",
    "    def get_stock_tickers(self):\n",
    "        \"\"\"\n",
    "        Returns a list of available stock tickers.\n",
    "\n",
    "        Returns:\n",
    "            (List[str]): A sorted list of available stock tickers.\n",
    "        \"\"\"\n",
    "        return sorted(self._get_tickers(self.stock_dir, self.file_suffix))\n",
    "\n",
    "    def get_all_tickers(self):\n",
    "        \"\"\"\n",
    "        Returns a list of available stock and ETF tickers.\n",
    "\n",
    "        Returns:\n",
    "            (List[str]): A sorted list of available stock and ETF tickers.\n",
    "        \"\"\"\n",
    "        return sorted(self.get_etf_tickers() + self.get_stock_tickers())\n",
    "\n",
    "    def get_csv_data(self, ticker, start_date=None, end_date=None):\n",
    "        \"\"\"\n",
    "        Returns a list of available stock and ETF tickers.\n",
    "\n",
    "        Args:\n",
    "            ticker (str): A ticker name.\n",
    "            start_date (str): A start date\n",
    "            end_date (str): An end date\n",
    "\n",
    "        Returns:\n",
    "            (pd.DataFrame): Parsed market data for a given ticker.\n",
    "        \"\"\"\n",
    "        ticker = ticker.lower()\n",
    "\n",
    "        # Check ETFs first since the number of potential hits is much smaller.\n",
    "        if ticker in self.get_etf_tickers():\n",
    "            dirname = self.etf_dir\n",
    "        else:\n",
    "            dirname = self.stock_dir\n",
    "\n",
    "        result = pd.DataFrame()\n",
    "        csv = pd.read_csv(os.path.join(dirname, ticker + self.file_suffix))\n",
    "        result = pd.DataFrame({\n",
    "            'date': csv['Date'],\n",
    "            'ticker': ticker,\n",
    "            'price': csv['Close'],\n",
    "            'volume': csv['Volume']\n",
    "        })\n",
    "        result['date'] = result['date'].map(\n",
    "            lambda t: datetime.strptime(t, '%Y-%m-%d')).to_numpy()\n",
    "\n",
    "        if start_date is not None:\n",
    "            result = result[result['date'] >= start_date]\n",
    "\n",
    "        if end_date is not None:\n",
    "            result = result[result['date'] <= end_date]\n",
    "\n",
    "        result.index = result.date\n",
    "\n",
    "        # Only including weekdays\n",
    "        result['dayofweek'] = pd.DatetimeIndex(result.index).dayofweek\n",
    "        result = result[(result.dayofweek != 5) & (result.dayofweek != 6)]\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_data = EquityData(DATA_DIR, STOCK_DIR, ETF_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out this API and load market data for 'SPY' (ETF tracking S&P 500 index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ticker = 'SPY'\n",
    "\n",
    "data = equity_data.get_csv_data(example_ticker)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure there is no overlap between between stock and ETF tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etf_tickers = equity_data.get_etf_tickers()\n",
    "stock_tickers = equity_data.get_stock_tickers()\n",
    "\n",
    "if len(np.intersect1d(etf_tickers, stock_tickers)) == 0:\n",
    "    print('No overlap - good')\n",
    "else:\n",
    "    print('There is some overlap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate some basic stats for each ticker. Given limited resources of my laptop I am going to analyze each file separately and aggregate results. Another approach would be to use Spark or Dask (something that we will try to do in the following section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats = []\n",
    "\n",
    "for ticker in tqdm(equity_data.get_all_tickers()):\n",
    "    data = equity_data.get_csv_data(ticker)\n",
    "    stats.append({\n",
    "        'ticker':\n",
    "        ticker,\n",
    "        'min_date':\n",
    "        data.index.min(),\n",
    "        'max_date':\n",
    "        data.index.max(),\n",
    "        'dollar_volume':\n",
    "        np.median(data['price'][-10:] * data['volume'][-10:])\n",
    "    })\n",
    "\n",
    "stats = pd.DataFrame(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_min_date = stats.groupby(\n",
    "    ['min_date'])['ticker'].count().reset_index().rename({'ticker': 'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the number of data points for each date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_data_points(ticker_list):\n",
    "    cnt = None\n",
    "    for ticker in tqdm(ticker_list):\n",
    "        data = equity_data.get_csv_data(ticker)\n",
    "        data['count'] = 1\n",
    "\n",
    "        if cnt is None:\n",
    "            cnt = data['count']\n",
    "        else:\n",
    "            cnt = cnt.add(data['count'], fill_value=0)\n",
    "\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnt_etf = count_data_points(equity_data.get_etf_tickers())\n",
    "cnt_stock = count_data_points(equity_data.get_stock_tickers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(cnt_etf.index, cnt_etf.values)\n",
    "plt.plot(cnt_stock.index, cnt_stock.values)\n",
    "plt.plot(cnt_min_date.min_date, np.cumsum(cnt_min_date.ticker))\n",
    "plt.title('Number of Daily OHLC Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The latest date available is {} and it contains {:,.0f} stocks and {:,.0f} ETFs'.format(\n",
    "    cnt_stock.index[-1], cnt_stock[-1], cnt_etf[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the amount of data points is generally growing and it contains a couple of extreme jumps. Given that the overall number of listed companies has been going down this most likely implies that the original download process was being actively extended to include additional tickers.\n",
    "\n",
    "In particular there is a huge jump taking place sometime in 2005 -- such a sharp jump would indicate that a process collecting data was extended to include a much larger universe of stocks and ETFs.\n",
    "\n",
    "Let's calculate daily percentage change in the number of per day tickers and check for anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_total = cnt_etf.add(cnt_stock, fill_value=0)\n",
    "cnt = pd.DataFrame({\n",
    "    'date': cnt_total.index,\n",
    "    'total': cnt_total.values\n",
    "},\n",
    "                   index=cnt_total.index)\n",
    "\n",
    "cnt['etf'] = cnt_etf\n",
    "cnt['etf'] = cnt['etf'].fillna(0)\n",
    "\n",
    "cnt['stock'] = cnt_stock\n",
    "cnt['stock'] = cnt['stock'].fillna(0)\n",
    "\n",
    "cnt['stock_cnt_pct_chg'] = np.abs(cnt['stock'].pct_change())\n",
    "cnt['etf_cnt_pct_chg'] = np.abs(cnt['etf'].pct_change())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnt[cnt.stock_cnt_pct_chg > 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt[cnt.etf_cnt_pct_chg > 0.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the two huge jumps observed on the chart took place on 2005-02-25 and 2011-02-17. Let's take a look at them in detail below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt['2005-02-23':'2005-02-28']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jump is probably due to the fact that the collection process was extended to include a much \n",
    "bigger universe of stocks and ETFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt['2011-02-16':'2011-02-22']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not seem to be a significant trading event taking place on this date (that would somehow affect trading for a large number of stocks and ETFs), therefore this drop is likely due to a one-day issue with the data collection process. In fact let's use Yahoo data to test this theory out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Yahoo Finance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2010-01-01'\n",
    "end_date = '2011-12-31'\n",
    "example_ticker = 'SPY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yh_data = web.DataReader(example_ticker, 'yahoo', start_date, end_date)\n",
    "yh_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = equity_data.get_csv_data(example_ticker, start_date, end_date)\n",
    "csv_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that calls return two identical sets of tradning dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yh_dates_not_csv = yh_data.index[~yh_data.index.isin(csv_data.index)]\n",
    "if len(yh_dates_not_csv):\n",
    "    print('Here is a list of dates returned by Yahoo but not found in csv: {}'.\n",
    "          format(list(yh_dates_not_csv.strftime('%Y-%m-%d'))))\n",
    "\n",
    "csv_dates_not_yh = csv_data.index[~csv_data.index.isin(yh_data.index)]\n",
    "if len(csv_dates_not_yh):\n",
    "    print('Here is a list of dates returned by Yahoo but not found in csv: {}'.\n",
    "          format(list(csv_dates_not_yh.strftime('%Y-%m-%d'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms our earlier theory that the csv data was not fully downloaded on '2011-02-17'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The csv data is supposed to be split & dividend adjusted as of '2017-11-10' vs how Yahoo adjusts its data based on the the latest available date. Let's download a new set of dates and compare csv prices vs unadjusted Yahoo prices. Let's use ratios instead of absolute differences to perform this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_date = '2016-01-01'\n",
    "end_date = '2017-11-10'\n",
    "\n",
    "yh_data = web.DataReader(example_ticker, 'yahoo', start_date, end_date)\n",
    "csv_data = equity_data.get_csv_data(example_ticker, start_date, end_date)\n",
    "\n",
    "abs(yh_data['Close'] / csv_data['price']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps (for the most part) are very smooth and imply that csv data was dividend adjusted (SPY does not have any splits).\n",
    "\n",
    "There are more checks that should be performed to ensure data quality of csv files, but let's stop here for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing Dask for Analyzing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that each ticker is saved into its own file, a more efficient (from usability perspective) would be to calculate basic statistics utilizing a tool like Dask. Here is an example how we could go about in case of analyzing stock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We map stock files to one virtual dataframe. A new column ('path') is created\n",
    "# which contains a source filename for each data row.\n",
    "\n",
    "STOCK_DATA_DIR = os.path.join(DATA_DIR, STOCK_DIR)\n",
    "stocks = dd.read_csv(os.path.join(STOCK_DATA_DIR,\n",
    "                                  '*' + EquityData.FILE_SUFFIX),\n",
    "                     include_path_column=True)\n",
    "\n",
    "# We extract a ticker using the path variable\n",
    "stocks['ticker'] = stocks['path'].str.replace('.*' + STOCK_DATA_DIR + '/',\n",
    "                                              '',\n",
    "                                              regex=True)\n",
    "stocks['ticker'] = stocks['ticker'].str.replace('.us.txt', '')\n",
    "\n",
    "stocks = stocks.drop(columns=['Open', 'High', 'Low', 'OpenInt', 'path'])\n",
    "stocks = stocks.rename(columns={\n",
    "    'Date': 'date',\n",
    "    'Close': 'price',\n",
    "    'Volume': 'volume'\n",
    "})\n",
    "stocks.index = stocks.date\n",
    "stocks = stocks[['ticker', 'price', 'volume']]\n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnt_stock_dask = stocks.groupby(['date'])['price'].count().compute()\n",
    "\n",
    "# We need to make sure that index values are sorted\n",
    "cnt_stock_dask = cnt_stock_dask.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all(cnt_stock == cnt_stock):\n",
    "    print('Results for both approaches (Dask & Pandas) are matching up - good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** In general, using Dask is much more convenient compared to writing custom functions to process data for each ticker and than aggregate results. One should run some performance checks to further examine pros and cons of using Dask."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
